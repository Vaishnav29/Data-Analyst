{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcd9997",
   "metadata": {},
   "source": [
    "# Webscraping Project from GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615d46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective is to get details of all top trending topics on Github\n",
    "# We need to get the Topic Name, Description and Topic Link on first page\n",
    "# We need to get details of top repositories of all top topics and create as csv file for each topic\n",
    "# We need username, repo name, star rating and link of each repository of each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5680b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries needed and rest as you work on\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80f61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are many functions in this code but the beggining is here even though this is not the first fucntion\n",
    "# This function helps us get all the top trending topics on the page of github\n",
    "\n",
    "def scrape_topics():\n",
    "    topics_url = \"https://github.com/topics\"\n",
    "    response = requests.get(topics_url)\n",
    "        \n",
    "    from bs4 import BeautifulSoup\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    topics_dict = {\n",
    "    'Titles' : get_topic_titles(doc),\n",
    "    'Descriptions' : get_topic_desc(doc),\n",
    "    'Links' : get_topic_links(doc) }\n",
    "  \n",
    "    return pd.DataFrame(topics_dict)\n",
    "\n",
    "\n",
    "#The below 3 functions are the helper functions to the above function to retrieve details of each topic\n",
    "\n",
    "def get_topic_titles(doc):\n",
    "    \n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p',{'class': selection_class})\n",
    "    topic_titles = []\n",
    "    \n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles\n",
    "\n",
    "\n",
    "def get_topic_desc(doc):\n",
    "    \n",
    "    desc_selector_class = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p',{'class': desc_selector_class})\n",
    "    topic_descs = []\n",
    "    \n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text)\n",
    "    return topic_descs\n",
    "\n",
    "def get_topic_links(doc):\n",
    "    \n",
    "    link_selector = 'no-underline flex-grow-0'\n",
    "    title_links = doc.find_all('a',{'class':link_selector})\n",
    "    topic_links = []\n",
    "    \n",
    "    for tag in title_links:\n",
    "        topic_links.append(\"https://github.com\"+tag['href'])\n",
    "    return topic_links\n",
    "\n",
    "\n",
    "#After all the above functions we will have a dictionary / data frame which contains top 30 Titles, descriptions and links of topics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881ae1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the second phase of execution\n",
    "#Now that we have topic names and URl's, we will scrape repo details of each topic through below function.\n",
    "\n",
    "def scrape_topic(path,topic_url):\n",
    "    import os\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists..so skipping \".format(path))\n",
    "        return\n",
    "        \n",
    "    topic_df = get_topic_repos(get_topic_doc(topic_url))\n",
    "    topic_df.to_csv(path,index = None)\n",
    "    \n",
    "    return topic_df\n",
    "\n",
    "#The final CSV files are created after the above function\n",
    "\n",
    "#The below helper function get us the data inside each link of topic and gives us all content.\n",
    "\n",
    "def get_topic_doc(topic_url):\n",
    "    \n",
    "    #Download page from link\n",
    "    response = requests.get(topic_url)\n",
    "    #Check for response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to load page {}\".format(topic_url))\n",
    "        pass\n",
    "    #Parse using beautifulsoup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return topic_doc\n",
    "\n",
    "#The below helper function get us the data from all the tags got from previous function\n",
    "\n",
    "def parse_star_count(star_tags):\n",
    "    if star_tags.text[-1] == 'k':\n",
    "        return int(float(star_tags.text[:-1])*1000)\n",
    "    return(int(star_tags.text))\n",
    "\n",
    "def get_repo_info(h3_tag,star_tags):\n",
    "    #returns all about repo\n",
    "    a_tags = h3_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = \"https://github.com\"+ a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tags)\n",
    "    \n",
    "    return username,repo_name,repo_url,stars\n",
    "\n",
    "\n",
    "def get_topic_repos(topic_doc):\n",
    "\n",
    "    #Get H3 tags which has username, repo name and repo url\n",
    "    username_tags = topic_doc.find_all('h3',{'class':'f3 color-fg-muted text-normal lh-condensed'})\n",
    "    \n",
    "    #get span tag which has stars\n",
    "    star_tags = topic_doc.find_all('span', {'class':'Counter js-social-count'})\n",
    "    \n",
    "    #Get repo infos\n",
    "    \n",
    "    topic_repos_dict = {\n",
    "    'username': [],\n",
    "    'repo_name':[],\n",
    "    'stars': [],\n",
    "    'repo_url':[]\n",
    "                        }\n",
    "\n",
    "    for i in range(len(username_tags)):\n",
    "        repo_info = get_repo_info(username_tags[i],star_tags[i])\n",
    "        topic_repos_dict[\"username\"].append(repo_info[0])\n",
    "        topic_repos_dict[\"repo_name\"].append(repo_info[1])\n",
    "        topic_repos_dict[\"stars\"].append(repo_info[3])\n",
    "        topic_repos_dict[\"repo_url\"].append(repo_info[2])\n",
    "    \n",
    "    \n",
    "    return(pd.DataFrame(topic_repos_dict))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c583886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the parent function which triggers the execution of the whole code\n",
    "\n",
    "def scrape_topics_repos():\n",
    "    import os\n",
    "\n",
    "    print(\"scraping list of topics\")\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('data', exist_ok = True)\n",
    "    \n",
    "    for index,row in topics_df.iterrows():\n",
    "        print(\"scraping top repositories for\"+row[\"Titles\"])\n",
    "        scrape_topic('data/'+ row['Titles']+'.csv',row['Links'])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc22735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping list of topics\n",
      "scraping top repositories for3D\n",
      "The file data/3D.csv already exists..so skipping \n",
      "scraping top repositories forAjax\n",
      "The file data/Ajax.csv already exists..so skipping \n",
      "scraping top repositories forAlgorithm\n",
      "The file data/Algorithm.csv already exists..so skipping \n",
      "scraping top repositories forAmp\n",
      "The file data/Amp.csv already exists..so skipping \n",
      "scraping top repositories forAndroid\n",
      "The file data/Android.csv already exists..so skipping \n",
      "scraping top repositories forAngular\n",
      "The file data/Angular.csv already exists..so skipping \n",
      "scraping top repositories forAnsible\n",
      "The file data/Ansible.csv already exists..so skipping \n",
      "scraping top repositories forAPI\n",
      "The file data/API.csv already exists..so skipping \n",
      "scraping top repositories forArduino\n",
      "The file data/Arduino.csv already exists..so skipping \n",
      "scraping top repositories forASP.NET\n",
      "The file data/ASP.NET.csv already exists..so skipping \n",
      "scraping top repositories forAtom\n",
      "The file data/Atom.csv already exists..so skipping \n",
      "scraping top repositories forAwesome Lists\n",
      "The file data/Awesome Lists.csv already exists..so skipping \n",
      "scraping top repositories forAmazon Web Services\n",
      "The file data/Amazon Web Services.csv already exists..so skipping \n",
      "scraping top repositories forAzure\n",
      "The file data/Azure.csv already exists..so skipping \n",
      "scraping top repositories forBabel\n",
      "The file data/Babel.csv already exists..so skipping \n",
      "scraping top repositories forBash\n",
      "The file data/Bash.csv already exists..so skipping \n",
      "scraping top repositories forBitcoin\n",
      "The file data/Bitcoin.csv already exists..so skipping \n",
      "scraping top repositories forBootstrap\n",
      "The file data/Bootstrap.csv already exists..so skipping \n",
      "scraping top repositories forBot\n",
      "The file data/Bot.csv already exists..so skipping \n",
      "scraping top repositories forC\n",
      "The file data/C.csv already exists..so skipping \n",
      "scraping top repositories forChrome\n",
      "The file data/Chrome.csv already exists..so skipping \n",
      "scraping top repositories forChrome extension\n",
      "The file data/Chrome extension.csv already exists..so skipping \n",
      "scraping top repositories forCommand line interface\n",
      "The file data/Command line interface.csv already exists..so skipping \n",
      "scraping top repositories forClojure\n",
      "The file data/Clojure.csv already exists..so skipping \n",
      "scraping top repositories forCode quality\n",
      "The file data/Code quality.csv already exists..so skipping \n",
      "scraping top repositories forCode review\n",
      "The file data/Code review.csv already exists..so skipping \n",
      "scraping top repositories forCompiler\n",
      "The file data/Compiler.csv already exists..so skipping \n",
      "scraping top repositories forContinuous integration\n",
      "The file data/Continuous integration.csv already exists..so skipping \n",
      "scraping top repositories forCOVID-19\n",
      "The file data/COVID-19.csv already exists..so skipping \n",
      "scraping top repositories forC++\n",
      "The file data/C++.csv already exists..so skipping \n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89083122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to get details all the pages of topics and all the repositories of each topic then...\n",
    "# The easiest hack is to loop over the scrape_topics_repos() function but by passing a values of number of pages of topics and number of pages of repositories to the respective functions\n",
    "# We need create a variable and add to the def scrape_topics(): topics_url = \"https://github.com/topics?page=<Variable>\n",
    "# We need to create a variable and add to the scrape_topics_repos() with row['Links']+'?page=<Variable>' and create loops\n",
    "# This is a project to show the surface of capabilties of webscraping and much more can be accomplished with more time.\n",
    "#I have done this project with the help of multiple tutorials and past experience in 6 hours of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
